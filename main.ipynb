{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List, Set, Tuple, Union, Generator\n",
    "from typing import OrderedDict as OrderedDictType\n",
    "from collections import OrderedDict\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Production:\n",
    "    \"\"\"\n",
    "    产生式规则\n",
    "    \"\"\"\n",
    "    left: str\n",
    "    right: List[str]\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.left} -> {' '.join(self.right)}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(str(self))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.__hash__() == other.__hash__()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Grammar:\n",
    "    \"\"\"\n",
    "    文法\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, productions: Set[Production], start_symbol: str):\n",
    "        self._productions: Set[Production] = productions  # P\n",
    "        self._terminals: Set[str] = set()  # V_T\n",
    "        self._non_terminals: Set[str] = set()  # V_N\n",
    "        self._start_symbol: str = start_symbol  # S\n",
    "\n",
    "        self._productions_by_key: OrderedDictType[str, List[Production]] = OrderedDict()  # A -> alpha_1 | ... | alpha_n\n",
    "\n",
    "        self._compute_non_terminals()\n",
    "        self._compute_terminals()\n",
    "        self._compute_productions_by_key()\n",
    "\n",
    "    @property\n",
    "    def productions_by_key(self):\n",
    "        if len(self._productions_by_key) == 0:\n",
    "            self._compute_productions_by_key()\n",
    "        return self._productions_by_key\n",
    "\n",
    "    @property\n",
    "    def productions(self):\n",
    "        return self._productions\n",
    "\n",
    "    @property\n",
    "    def terminals(self):\n",
    "        return self._terminals\n",
    "\n",
    "    @property\n",
    "    def non_terminals(self):\n",
    "        return self._non_terminals\n",
    "\n",
    "    @property\n",
    "    def start_symbol(self):\n",
    "        return self._start_symbol\n",
    "\n",
    "    def _compute_productions_by_key(self):\n",
    "        \"\"\"\n",
    "        create a dictionary for easy access, i.e. [A -> B,..., A -> C, A -> D] => A: A -> B | C | D, B: B->...\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        for production in self._productions:\n",
    "            if production.left not in self._productions_by_key:\n",
    "                self._productions_by_key[production.left] = []\n",
    "            self._productions_by_key[production.left].append(production)\n",
    "\n",
    "    def _compute_non_terminals(self):\n",
    "        \"\"\"\n",
    "        计算非终结符集合\n",
    "        \"\"\"\n",
    "        for production in self._productions:\n",
    "            self._non_terminals.add(production.left)\n",
    "\n",
    "    def _compute_terminals(self):\n",
    "        \"\"\"\n",
    "        计算终结符集合\n",
    "        \"\"\"\n",
    "        if len(self._non_terminals) == 0:\n",
    "            self._compute_non_terminals()\n",
    "        for production in self._productions:\n",
    "            for symbol in production.right:\n",
    "                if symbol not in self._non_terminals:\n",
    "                    self._terminals.add(symbol)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"\"\"\n",
    "Start Symbol: {self._start_symbol}\n",
    "Terminals: {self._terminals}\n",
    "Non-terminals: {self._non_terminals}\n",
    "Productions:\n",
    "\"\"\" + \"\\n\".join([str(p) for p in self._productions])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(str(self))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def load_grammar(file_path: Union[Path,str]) -> Grammar:\n",
    "    production_regex = re.compile(r'(?P<left>.+) -> (?P<right>.+)')\n",
    "    productions: List[Production] = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()  # remove trailing whitespace\n",
    "            if line == \"\":\n",
    "                continue\n",
    "            match = production_regex.match(line)\n",
    "            if match is None:\n",
    "                raise Exception(f\"Invalid production: {line}\")\n",
    "            left = match.group(\"left\")\n",
    "            right = match.group(\"right\").split(\" \")\n",
    "            productions.append(Production(left, right))\n",
    "\n",
    "    grammar = Grammar(productions=set(productions),\n",
    "                      start_symbol=productions[0].left)\n",
    "    return grammar"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "grammar = load_grammar(\"grammar.txt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start Symbol: program\n",
      "Terminals: {')', '-', '=', 'void', '<=', '$', '!=', '+', '/', '*', '%', 'INT', 'IDN', '{', 'return', '==', '<', '>=', ';', '(', '}', ',', '>', 'const', 'int'}\n",
      "Non-terminals: {'decl', 'funcFParam', 'argConst', 'argFunctionF', 'block', 'argFunctionR', 'addExpAtom', 'argExp', 'bType', 'stmt', 'assignExpAtom', 'compUnit', 'blockItem', 'constDef', 'funcRParam', 'initVal', 'number', 'funcType', 'assignExp', 'argVarDef', 'mulExpAtom', 'exp', 'program', 'eqExp', 'mulExp', 'relExpAtom', 'funcDef', 'callFunc', 'funcRParams', 'eqExpAtom', 'constExp', 'constInitVal', 'relExp', 'funcFParams', 'addExp', 'unaryExp', 'varDef', 'argVarDecl', 'varDecl', 'constDecl'}\n",
      "Productions:\n",
      "relExpAtom -> >= addExp relExpAtom\n",
      "unaryExp -> IDN callFunc\n",
      "relExpAtom -> <= addExp relExpAtom\n",
      "addExpAtom -> - mulExp addExpAtom\n",
      "unaryExp -> number\n",
      "compUnit -> $\n",
      "funcRParam -> exp\n",
      "assignExpAtom -> = eqExp assignExpAtom\n",
      "number -> INT\n",
      "argVarDecl -> $\n",
      "argFunctionR -> , funcRParam argFunctionR\n",
      "eqExpAtom -> == relExp eqExpAtom\n",
      "stmt -> block\n",
      "funcFParam -> bType IDN\n",
      "compUnit -> funcDef compUnit\n",
      "exp -> assignExp\n",
      "mulExpAtom -> % unaryExp mulExpAtom\n",
      "relExpAtom -> $\n",
      "funcRParams -> $\n",
      "relExpAtom -> < addExp relExpAtom\n",
      "compUnit -> decl compUnit\n",
      "bType -> int\n",
      "stmt -> ;\n",
      "assignExp -> eqExp assignExpAtom\n",
      "eqExpAtom -> $\n",
      "initVal -> exp\n",
      "blockItem -> stmt blockItem\n",
      "constDef -> IDN = constInitVal\n",
      "funcDef -> funcType IDN ( funcFParams ) block\n",
      "argConst -> , constDef argConst\n",
      "callFunc -> ( funcRParams )\n",
      "callFunc -> $\n",
      "constInitVal -> constExp\n",
      "addExp -> mulExp addExpAtom\n",
      "program -> compUnit\n",
      "varDecl -> bType varDef argVarDecl ;\n",
      "argVarDef -> = initVal\n",
      "blockItem -> $\n",
      "argExp -> exp\n",
      "eqExp -> relExp eqExpAtom\n",
      "argVarDef -> $\n",
      "eqExpAtom -> != relExp eqExpAtom\n",
      "argVarDecl -> , varDef argVarDecl\n",
      "blockItem -> decl blockItem\n",
      "argFunctionR -> $\n",
      "relExp -> addExp relExpAtom\n",
      "decl -> constDecl\n",
      "addExpAtom -> $\n",
      "mulExpAtom -> $\n",
      "relExpAtom -> > addExp relExpAtom\n",
      "assignExpAtom -> $\n",
      "addExpAtom -> + mulExp addExpAtom\n",
      "argFunctionF -> , funcFParam argFunctionF\n",
      "stmt -> exp ;\n",
      "constDecl -> const bType constDef argConst ;\n",
      "funcType -> void\n",
      "mulExpAtom -> * unaryExp mulExpAtom\n",
      "argExp -> $\n",
      "mulExp -> unaryExp mulExpAtom\n",
      "funcFParams -> funcFParam argFunctionF\n",
      "varDef -> IDN argVarDef\n",
      "constExp -> assignExp\n",
      "funcFParams -> $\n",
      "stmt -> return argExp ;\n",
      "funcRParams -> funcRParam argFunctionR\n",
      "mulExpAtom -> / unaryExp mulExpAtom\n",
      "decl -> varDecl\n",
      "argConst -> $\n",
      "block -> { blockItem }\n",
      "argFunctionF -> $\n"
     ]
    }
   ],
   "source": [
    "print(grammar)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Left-Recursion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def eliminate_left_recursion(grammar: Grammar) -> Grammar:\n",
    "    \"\"\"\n",
    "    消除左递归：\n",
    "        1. 带入生成式，产生 mid_productions_p_i\n",
    "        2. 消除 mid_productions_p_i 的直接左递归，产生无左递归的 new_productions_p_i\n",
    "    Args:\n",
    "        grammar:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    productions = list(grammar.productions)\n",
    "    terminals = list(grammar.terminals)\n",
    "    non_terminals = list(grammar.non_terminals)\n",
    "    new_productions: List[Production] = []\n",
    "\n",
    "    def get_right_symbols(productions_list: List[Production]) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Get symbols that appeared on the right hand side of a production\n",
    "        Args:\n",
    "            productions_list:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        right_symbol_set = set()\n",
    "        for p in productions_list:\n",
    "            for sym in p.right:\n",
    "                right_symbol_set.add(sym)\n",
    "        return right_symbol_set\n",
    "\n",
    "    # create a dictionary for easy access, i.e. [A -> B,..., A -> C, A -> D] => A: A -> B | C | D, B: B->...\n",
    "    productions_by_key: OrderedDictType[\n",
    "        str, List[Production]] = grammar.productions_by_key  # A -> alpha_1 | ... | alpha_n\n",
    "    for production in productions:\n",
    "        if production.left not in productions_by_key:\n",
    "            productions_by_key[production.left] = []\n",
    "        productions_by_key[production.left].append(production)\n",
    "\n",
    "    # indirect left recursion\n",
    "    for i in range(len(non_terminals)):\n",
    "        P_i = non_terminals[i]\n",
    "\n",
    "        prev_productions_p_i = productions_by_key[P_i]\n",
    "        # intermediate productions, (not recursion free). Those start with terminals is not changed\n",
    "        mid_productions_p_i: List[Production] = [p for p in prev_productions_p_i if p.right[0] in terminals]\n",
    "        # those with terminals after P_i is not changed\n",
    "        mid_productions_p_i.extend([p for p in prev_productions_p_i if p.right[0] in non_terminals[i:]])\n",
    "\n",
    "        for j in range(i):\n",
    "            P_j = non_terminals[j]\n",
    "\n",
    "            # find all right hand side symbols of P_i\n",
    "            P_i_right_symbol_set = get_right_symbols(productions_by_key[P_i])\n",
    "\n",
    "            if P_j not in P_i_right_symbol_set:  # P_i -> P_j gamma not exists\n",
    "                continue\n",
    "\n",
    "            # P_i -> P_j gamma exists,\n",
    "            # change P_i -> P_j gamma to P_i -> delta_1 gamma | delta_2 gamma | ... | delta_n gamma\n",
    "            # where P_j -> delta_1 | delta_2 | ... | delta_n\n",
    "            productions_p_j = productions_by_key[P_j]\n",
    "            for production_p_i in prev_productions_p_i:  # for each in P_i -> alpha_1 | alpha_2 | ... | alpha_n\n",
    "                if production_p_i.right[0] == P_j:  # if production_p_i: P_i -> P_j gamma\n",
    "                    gamma = production_p_i.right[1:]\n",
    "                    for production_p_j in productions_p_j:  # for each in P_j -> delta_1 | ... | delta_n\n",
    "                        delta = production_p_j.right\n",
    "                        mid_production_p_i = Production(\n",
    "                            left=P_i,\n",
    "                            right=delta + gamma\n",
    "                        )  # P_i -> delta gamma\n",
    "                        mid_productions_p_i.append(mid_production_p_i)\n",
    "\n",
    "                else:  # production_p_i: P_i -> beta\n",
    "                    mid_productions_p_i.append(production_p_i)\n",
    "\n",
    "            productions_by_key[P_i] = mid_productions_p_i  # TODO check\n",
    "            prev_productions_p_i = productions_by_key[P_i]\n",
    "\n",
    "            # store intermediate productions. Those start with terminals is not changed\n",
    "            mid_productions_p_i: List[Production] = [p for p in prev_productions_p_i if p.right[0] in terminals]\n",
    "            # those with terminals after P_i is not changed\n",
    "            mid_productions_p_i.extend([p for p in prev_productions_p_i if p.right[0] in non_terminals[i:]])\n",
    "\n",
    "        # end of for j in range(i)\n",
    "\n",
    "        # attempt to eliminate direct left recursion for P_i\n",
    "        new_productions_p_i: List[Production] = []  # stores left-recursion-free productions for P_i\n",
    "        new_productions_p_i_: List[Production] = []  # P_i' for left recursion elimination\n",
    "\n",
    "        # test if P_i has left recursion\n",
    "        has_left_recursion: bool = False\n",
    "        for production_p_i in mid_productions_p_i:\n",
    "            if production_p_i.right[0] == P_i:  # production_p_i: P_i -> P_i gamma\n",
    "                print(f\"left-recursion found! {production_p_i}\")\n",
    "                has_left_recursion = True\n",
    "                break\n",
    "\n",
    "        if not has_left_recursion:\n",
    "            new_productions_p_i = mid_productions_p_i\n",
    "        else:\n",
    "            # eliminate direct left recursion for P_i\n",
    "            for production_p_i in mid_productions_p_i:\n",
    "                if production_p_i.right[0] == P_i:  # production_p_i: P_i -> P_i gamma\n",
    "                    gamma = production_p_i.right[1:]\n",
    "\n",
    "                    # P_i' -> epsilon | gamma P_i'\n",
    "                    new_production_p_i_ = Production(\n",
    "                        left=P_i + \"'\",\n",
    "                        right=gamma + [P_i + \"'\"]\n",
    "                    )  # P_i' -> gamma P_i'\n",
    "                    new_productions_p_i_.append(new_production_p_i_)\n",
    "                else:  # production_p_i: P_i -> beta, no left recursion for current production\n",
    "                    new_production_p_i = Production(\n",
    "                        left=P_i,\n",
    "                        right=production_p_i.right + [P_i + \"'\"]\n",
    "                    )  # P_i -> beta P_i'\n",
    "                    new_productions_p_i.append(new_production_p_i)\n",
    "\n",
    "            # add P_i' -> epsilon\n",
    "            new_productions_p_i_.append(Production(\n",
    "                left=P_i + \"'\",\n",
    "                right=[\"$\"]\n",
    "            ))  # P_i' -> epsilon\n",
    "\n",
    "            productions_by_key[P_i + \"'\"] = new_productions_p_i_\n",
    "\n",
    "        productions_by_key[P_i] = new_productions_p_i\n",
    "    # end of for i in range(len(non_terminals))\n",
    "\n",
    "    for k, v in productions_by_key.items():\n",
    "        new_productions.extend(v)\n",
    "\n",
    "    return Grammar(\n",
    "        productions=set(new_productions),\n",
    "        start_symbol=grammar.start_symbol\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Unit test for eliminate_left_recursion\n",
    "# \"\"\"\n",
    "#\n",
    "# old_grammar = load_grammar(\"unittest/grammar_recursion.txt\")\n",
    "# new_grammar = eliminate_left_recursion(old_grammar)\n",
    "# sorted(list(new_grammar.productions), key=lambda x: x.left)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "new_grammar = eliminate_left_recursion(grammar)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FIRST, FOLLOW"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### FIRST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def get_all_first(grammar: Grammar) -> Dict[str, Set[str]]:\n",
    "    \"\"\"\n",
    "    Get FIRST set of all symbols\n",
    "    Args:\n",
    "        grammar: a grammar\n",
    "\n",
    "    Returns: a dictionary of FIRST sets\n",
    "\n",
    "    \"\"\"\n",
    "    first: Dict[str, Set[str]] = {}\n",
    "    terminals = list(grammar.terminals)\n",
    "    non_terminals = list(grammar.non_terminals)\n",
    "\n",
    "    productions_by_key = grammar.productions_by_key\n",
    "\n",
    "    def get_first(symbol: str) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Get FIRST set of a symbol\n",
    "        Args:\n",
    "            symbol: a terminal or non-terminal symbol\n",
    "\n",
    "        Returns: a set of terminals\n",
    "\n",
    "        \"\"\"\n",
    "        if first.get(symbol) is not None:  # already computed\n",
    "            return first[symbol]\n",
    "\n",
    "        if symbol in terminals:  # terminal\n",
    "            return {symbol}\n",
    "        elif symbol in non_terminals:\n",
    "            first_set: Set[str] = set()\n",
    "            for production in productions_by_key[symbol]:\n",
    "                if production.right[0] == \"$\":  # production: symbol -> $\n",
    "                    first_set.add(\"$\")\n",
    "                else:\n",
    "                    for i in range(len(production.right)):\n",
    "                        right_first = get_first(production.right[i])\n",
    "                        if \"$\" in right_first:\n",
    "                            first_set.update(right_first - {\"$\"})\n",
    "                            if i == len(production.right) - 1:  # last symbol\n",
    "                                first_set.add(\"$\")\n",
    "                        else:\n",
    "                            first_set.update(right_first)\n",
    "                            break\n",
    "            return first_set\n",
    "        else:\n",
    "            raise Exception(f\"Invalid symbol: {symbol}\")\n",
    "\n",
    "    for terminal in terminals:\n",
    "        first[terminal] = {terminal}\n",
    "    for non_terminal in non_terminals:\n",
    "        first[non_terminal] = get_first(non_terminal)\n",
    "    return first"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "{')': {')'},\n '-': {'-'},\n '=': {'='},\n 'void': {'void'},\n '<=': {'<='},\n '$': {'$'},\n '!=': {'!='},\n '+': {'+'},\n '/': {'/'},\n '*': {'*'},\n '%': {'%'},\n 'INT': {'INT'},\n 'IDN': {'IDN'},\n '{': {'{'},\n 'return': {'return'},\n '==': {'=='},\n '<': {'<'},\n '>=': {'>='},\n ';': {';'},\n '(': {'('},\n '}': {'}'},\n ',': {','},\n '>': {'>'},\n 'const': {'const'},\n 'int': {'int'},\n 'decl': {'const', 'int'},\n 'funcFParam': {'int'},\n 'argConst': {'$', ','},\n 'argFunctionF': {'$', ','},\n 'block': {'{'},\n 'argFunctionR': {'$', ','},\n 'addExpAtom': {'$', '+', '-'},\n 'argExp': {'$', 'IDN', 'INT'},\n 'bType': {'int'},\n 'stmt': {';', 'IDN', 'INT', 'return', '{'},\n 'assignExpAtom': {'$', '='},\n 'compUnit': {'$', 'const', 'int', 'void'},\n 'blockItem': {'$', ';', 'IDN', 'INT', 'const', 'int', 'return', '{'},\n 'constDef': {'IDN'},\n 'funcRParam': {'IDN', 'INT'},\n 'initVal': {'IDN', 'INT'},\n 'number': {'INT'},\n 'funcType': {'void'},\n 'assignExp': {'IDN', 'INT'},\n 'argVarDef': {'$', '='},\n 'mulExpAtom': {'$', '%', '*', '/'},\n 'exp': {'IDN', 'INT'},\n 'program': {'$', 'const', 'int', 'void'},\n 'eqExp': {'IDN', 'INT'},\n 'mulExp': {'IDN', 'INT'},\n 'relExpAtom': {'$', '<', '<=', '>', '>='},\n 'funcDef': {'void'},\n 'callFunc': {'$', '('},\n 'funcRParams': {'$', 'IDN', 'INT'},\n 'eqExpAtom': {'!=', '$', '=='},\n 'constExp': {'IDN', 'INT'},\n 'constInitVal': {'IDN', 'INT'},\n 'relExp': {'IDN', 'INT'},\n 'funcFParams': {'$', 'int'},\n 'addExp': {'IDN', 'INT'},\n 'unaryExp': {'IDN', 'INT'},\n 'varDef': {'IDN'},\n 'argVarDecl': {'$', ','},\n 'varDecl': {'int'},\n 'constDecl': {'const'}}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FIRST = get_all_first(grammar)\n",
    "FIRST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### FOLLOW"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def get_all_follow(grammar: Grammar, FIRST: Dict[str, Set[str]]) -> Dict[str, Set[str]]:\n",
    "    \"\"\"\n",
    "    Get FOLLOW set of all symbols, iterative implementation\n",
    "    Args:\n",
    "        FIRST: first set\n",
    "        grammar: a grammar\n",
    "\n",
    "    Returns: a dictionary of FOLLOW sets\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    follow: Dict[str, Set[str]] = dict()\n",
    "    prev_follow: Dict[str, Set[str]] = None\n",
    "    non_terminals = list(grammar.non_terminals)\n",
    "    productions = list(grammar.productions)\n",
    "\n",
    "    for non_terminal in non_terminals:\n",
    "        follow[non_terminal] = set()\n",
    "\n",
    "    follow[grammar.start_symbol] = {\"EOF\"} # start symbol\n",
    "\n",
    "\n",
    "    while prev_follow != follow:\n",
    "        prev_follow = deepcopy(follow)\n",
    "        for p in productions:\n",
    "            beta_has_epsilon: bool = True\n",
    "            for i in reversed(range(len(p.right))):  # from back to front, to check for A -> alpha B beta, if beta =>* epsilon\n",
    "                if p.right[i] not in non_terminals:  # B is terminal\n",
    "                    continue\n",
    "\n",
    "                if i == len(p.right) - 1: # A -> alpha B, add FOLLOW(A) to FOLLOW(B)\n",
    "                    follow[p.right[i]].update(follow[p.left])\n",
    "                    # print(f\"FOLLOW({p.left}) -> FOLLOW({p.right[i]})\")\n",
    "                elif \"$\" in FIRST[p.right[i+1]] and beta_has_epsilon:  # beta =>* epsilon\n",
    "                    # print(f\"FOLLOW({p.left}) -> FOLLOW({p.right[i]})\")\n",
    "                    follow[p.right[i]].update(follow[p.left])\n",
    "                    follow[p.right[i]].update(FIRST[p.right[i+1]] - {\"$\"})\n",
    "                    # print(f\"FIRST({p.right[i+1]}) - {{\\\"$\\\"}} -> FOLLOW({p.right[i]})\")\n",
    "                else:\n",
    "                    beta_has_epsilon = False\n",
    "                    follow[p.right[i]].update(FIRST[p.right[i+1]] - {\"$\"})\n",
    "    return follow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# def get_all_follow(grammar: Grammar, FIRST: Dict[str, Set[str]]) -> Dict[str, Set[str]]:\n",
    "#     \"\"\"\n",
    "#     Get FOLLOW set of all symbols, recursive implementation\n",
    "#     Args:\n",
    "#         FIRST: first set\n",
    "#         grammar: a grammar\n",
    "#\n",
    "#     Returns: a dictionary of FOLLOW sets\n",
    "#\n",
    "#     \"\"\"\n",
    "#\n",
    "#     follow: Dict[str, Set[str]] = {}\n",
    "#     non_terminals = list(grammar.non_terminals)\n",
    "#\n",
    "#     def get_follow(symbol: str) -> Set[str]:\n",
    "#         \"\"\"\n",
    "#         Get FOLLOW set of a symbol\n",
    "#         Args:\n",
    "#             symbol: a terminal or non-terminal symbol\n",
    "#\n",
    "#         Returns: a set of terminals\n",
    "#\n",
    "#         \"\"\"\n",
    "#         if follow.get(symbol) is None:\n",
    "#             follow[symbol] = set()\n",
    "#\n",
    "#         for p in grammar.productions:\n",
    "#             beta_has_epsilon: bool = True\n",
    "#             for i in reversed(range(len(p.right))):  # for all occurrences of symbol on right hand side\n",
    "#\n",
    "#                 if i < len(p.right) - 1 and \"$\" not in FIRST[p.right[i + 1]]:  # not last, beta =>* epsilon is not possible\n",
    "#                     beta_has_epsilon = False\n",
    "#\n",
    "#                 if p.right[i] != symbol:  # not production: A -> alpha symbol beta\n",
    "#                     continue\n",
    "#\n",
    "#                 if i == len(p.right) - 1: # last symbol, A -> alpha B, add FOLLOW(A) to FOLLOW(B)\n",
    "#                     if p.left != symbol:  # avoid infinite recursion, A -> alpha A, FOLLOW(A)=FOLLOW(A)\n",
    "#                         follow[symbol].update(get_follow(p.left))\n",
    "#                 else:  # not last symbol, FOLLOW(symbol) += FIRST(beta) - {epsilon}\n",
    "#                     beta = p.right[i + 1]\n",
    "#                     follow[symbol].update(FIRST[beta] - {\"$\"})\n",
    "#                     if beta_has_epsilon and p.left != symbol: # beta =>* epsilon, A -> alpha B beta, add FOLLOW(A) to FOLLOW(B)\n",
    "#                         follow[symbol].update(get_follow(p.left))\n",
    "#\n",
    "#         return follow[symbol]\n",
    "#\n",
    "#     follow[grammar.start_symbol] = {\"EOF\"}\n",
    "#     for non_terminal in non_terminals:\n",
    "#         follow[non_terminal] = get_follow(non_terminal)\n",
    "#     return follow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "{'decl': {';',\n  'EOF',\n  'IDN',\n  'INT',\n  'const',\n  'int',\n  'return',\n  'void',\n  '{',\n  '}'},\n 'funcFParam': {')', ','},\n 'argConst': {';'},\n 'argFunctionF': {')'},\n 'block': {';',\n  'EOF',\n  'IDN',\n  'INT',\n  'const',\n  'int',\n  'return',\n  'void',\n  '{',\n  '}'},\n 'argFunctionR': {')'},\n 'addExpAtom': {'!=', ')', ',', ';', '<', '<=', '=', '==', '>', '>='},\n 'argExp': {';'},\n 'bType': {'IDN'},\n 'stmt': {';', 'IDN', 'INT', 'const', 'int', 'return', '{', '}'},\n 'assignExpAtom': {')', ',', ';'},\n 'compUnit': {'EOF'},\n 'blockItem': {'}'},\n 'constDef': {',', ';'},\n 'funcRParam': {')', ','},\n 'initVal': {',', ';'},\n 'number': {'!=',\n  '%',\n  ')',\n  '*',\n  '+',\n  ',',\n  '-',\n  '/',\n  ';',\n  '<',\n  '<=',\n  '=',\n  '==',\n  '>',\n  '>='},\n 'funcType': {'IDN'},\n 'assignExp': {')', ',', ';'},\n 'argVarDef': {',', ';'},\n 'mulExpAtom': {'!=',\n  ')',\n  '+',\n  ',',\n  '-',\n  ';',\n  '<',\n  '<=',\n  '=',\n  '==',\n  '>',\n  '>='},\n 'exp': {')', ',', ';'},\n 'program': {'EOF'},\n 'eqExp': {')', ',', ';', '='},\n 'mulExp': {'!=', ')', '+', ',', '-', ';', '<', '<=', '=', '==', '>', '>='},\n 'relExpAtom': {'!=', ')', ',', ';', '=', '=='},\n 'funcDef': {'EOF', 'const', 'int', 'void'},\n 'callFunc': {'!=',\n  '%',\n  ')',\n  '*',\n  '+',\n  ',',\n  '-',\n  '/',\n  ';',\n  '<',\n  '<=',\n  '=',\n  '==',\n  '>',\n  '>='},\n 'funcRParams': {')'},\n 'eqExpAtom': {')', ',', ';', '='},\n 'constExp': {',', ';'},\n 'constInitVal': {',', ';'},\n 'relExp': {'!=', ')', ',', ';', '=', '=='},\n 'funcFParams': {')'},\n 'addExp': {'!=', ')', ',', ';', '<', '<=', '=', '==', '>', '>='},\n 'unaryExp': {'!=',\n  '%',\n  ')',\n  '*',\n  '+',\n  ',',\n  '-',\n  '/',\n  ';',\n  '<',\n  '<=',\n  '=',\n  '==',\n  '>',\n  '>='},\n 'varDef': {',', ';'},\n 'argVarDecl': {';'},\n 'varDecl': {';',\n  'EOF',\n  'IDN',\n  'INT',\n  'const',\n  'int',\n  'return',\n  'void',\n  '{',\n  '}'},\n 'constDecl': {';',\n  'EOF',\n  'IDN',\n  'INT',\n  'const',\n  'int',\n  'return',\n  'void',\n  '{',\n  '}'}}"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FOLLOW = get_all_follow(grammar, FIRST)\n",
    "FOLLOW"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Unit tests for FIRST and FOLLOW\n",
    "# \"\"\"\n",
    "# grammar_first_follow = load_grammar(\"unittest/grammar_first_follow.txt\")\n",
    "# grammar_first_follow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# FIRST = get_all_first(grammar_first_follow)\n",
    "# FIRST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# FOLLOW = get_all_follow(grammar_first_follow, FIRST)\n",
    "# FOLLOW"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LL(1) Parsing Table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def get_parsing_table(productions, FIRST, FOLLOW) -> Dict[Tuple[str, str], Production]:\n",
    "    parse_table: Dict[Tuple[str, str], Production] = dict()\n",
    "\n",
    "    for p in productions:  # A -> alpha\n",
    "        A = p.left\n",
    "\n",
    "        # get FIRST(alpha)\n",
    "        FIRST_alpha = set()\n",
    "        if p.right[0] == \"$\":\n",
    "            FIRST_alpha.update(FOLLOW[p.left])\n",
    "        else:\n",
    "            FIRST_alpha.update(FIRST[p.right[0]])\n",
    "\n",
    "        for a in list(FIRST_alpha):\n",
    "            parse_table[(A, a)] = p\n",
    "            if \"$\" in FIRST_alpha:\n",
    "                for b in FOLLOW[A]:\n",
    "                    parse_table[(A, b)] = Production(A,\"$\")\n",
    "    return parse_table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "parsing_table = get_parsing_table(grammar.productions, FIRST, FOLLOW)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "import sys\n",
    "\n",
    "def print_parsing_table(grammar: Grammar, parsing_table, f=sys.stdout):\n",
    "    terminals = list(grammar.terminals)\n",
    "    non_terminals = list(grammar.non_terminals)\n",
    "\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"\"] + terminals + [\"EOF\"]\n",
    "    for non_terminal in non_terminals:\n",
    "        row = [non_terminal]\n",
    "        for terminal in terminals:\n",
    "            production = parsing_table.get((non_terminal, terminal))\n",
    "            if production is None:\n",
    "                row.append(\"\")\n",
    "            else:\n",
    "                row.append(str(production))\n",
    "        production = parsing_table.get((non_terminal, \"EOF\"))\n",
    "        if production is None:\n",
    "            row.append(\"\")\n",
    "        else:\n",
    "            row.append(str(production))\n",
    "        table.add_row(row)\n",
    "    print(table, file=f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "with open(\"output/parsing_table.txt\", \"w\") as f:\n",
    "    print_parsing_table(grammar, parsing_table, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LL(1) Parser"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class Stack:\n",
    "    def __init__(self):\n",
    "        self.stack = []\n",
    "\n",
    "    def push(self, item):\n",
    "        self.stack.append(item)\n",
    "\n",
    "    def pop(self) -> str:\n",
    "        return self.stack.pop()\n",
    "\n",
    "    def top(self) -> str:\n",
    "        return self.stack[-1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stack)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \" \".join([str(s) for s in reversed(self.stack)])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def input_generator(infile: Union[Path,str]) -> Generator[Tuple[str,str], None, None]:\n",
    "    with open(infile) as f:\n",
    "        lines = f.readlines()\n",
    "    lex_regex = re.compile(r\"(?P<lexeme>.+)\\s+<(?P<type>.+)>\")\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        match = lex_regex.match(line)\n",
    "        if match:\n",
    "            token_type = match.group(\"type\")\n",
    "            if not(token_type == \"INT\" or token_type == \"IDN\"):\n",
    "                token_type = match.group(\"lexeme\")\n",
    "            yield token_type, match.group(\"lexeme\")\n",
    "        else:\n",
    "            raise Exception(f\"Invalid input: {line}\")\n",
    "    yield \"EOF\", \"EOF\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# for token, lexeme in input_generator(\"testcases/00/00_lexical.txt\"):\n",
    "#     print(f\"{token} {lexeme}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class ParseSyntaxError(Exception):\n",
    "    ...\n",
    "\n",
    "def parse(grammar: Grammar, input_generator: Generator[Tuple[str,str], None, None]) -> Generator[str, None, None]:\n",
    "    \"\"\"\n",
    "    Parse input using LL(1) parsing table\n",
    "    Args:\n",
    "        grammar: a grammar\n",
    "        input_generator: a generator of input tokens\n",
    "\n",
    "    Returns: None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    FIRST = get_all_first(grammar)\n",
    "\n",
    "    FOLLOW = get_all_follow(grammar, FIRST)\n",
    "\n",
    "    parsing_table = get_parsing_table(grammar.productions, FIRST, FOLLOW)\n",
    "\n",
    "    stack = Stack()\n",
    "    stack.push(\"EOF\")\n",
    "    stack.push(grammar.start_symbol)\n",
    "    token, _ = next(input_generator)\n",
    "    while True:\n",
    "        # print(f\"Stack: {stack}\\t\", end=\"\\t\")\n",
    "        # print(f\"Input: {token}\\t\")\n",
    "        top = stack.top()\n",
    "        if top in grammar.terminals:\n",
    "            if top == token:  # if match, pop stack and get next token\n",
    "                # print(f\"Matched {token}\")\n",
    "                yield f\"{top}#{token}\\tmove\"\n",
    "\n",
    "                stack.pop()\n",
    "                try:\n",
    "                    token, _ = next(input_generator)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "            else:\n",
    "                yield f\"{top}#{token}\\terror\"\n",
    "                raise ParseSyntaxError(f\"Syntax error: expected {top} but got {token}\")\n",
    "        elif top in grammar.non_terminals:  # apply production\n",
    "            production = parsing_table.get((top, token))\n",
    "            if production is None:\n",
    "                yield f\"{top}#{token}\\terror\"\n",
    "                raise ParseSyntaxError(f\"Syntax error: no production for {top} {token}\")\n",
    "            # print(f\"Apply production {production}\")\n",
    "            yield f\"{top}#{token}\\treduction\"\n",
    "\n",
    "            stack.pop()\n",
    "            if production.right[0] != \"$\": # not A -> epsilon\n",
    "                for symbol in reversed(production.right):\n",
    "                    stack.push(symbol)\n",
    "        elif top == \"EOF\":\n",
    "            if top == token:\n",
    "                print(\"Accepted\")\n",
    "                break\n",
    "            else:\n",
    "                yield f\"{top}#{token}\\terror\"\n",
    "                raise ParseSyntaxError(f\"Syntax error: expected {top} but got {token}\")\n",
    "        else:\n",
    "            yield f\"{top}#{token}\\terror\"\n",
    "            raise ParseSyntaxError(f\"Invalid symbol: {top}\")\n",
    "\n",
    "    if stack.top() != \"EOF\":\n",
    "        yield f\"{top}#{token}\\terror\"\n",
    "        raise ParseSyntaxError(\"Syntax error: input is not fully parsed\")\n",
    "\n",
    "    yield \"EOF#EOF\\taccept\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program#int\treduction\n",
      "compUnit#int\treduction\n",
      "decl#int\treduction\n",
      "varDecl#int\treduction\n",
      "bType#int\treduction\n",
      "int#int\tmove\n",
      "varDef#IDN\treduction\n",
      "IDN#IDN\tmove\n",
      "argVarDef#=\treduction\n",
      "=#=\tmove\n",
      "initVal#INT\treduction\n",
      "exp#INT\treduction\n",
      "assignExp#INT\treduction\n",
      "eqExp#INT\treduction\n",
      "relExp#INT\treduction\n",
      "addExp#INT\treduction\n",
      "mulExp#INT\treduction\n",
      "unaryExp#INT\treduction\n",
      "number#INT\treduction\n",
      "INT#INT\tmove\n",
      "mulExpAtom#void\terror\n"
     ]
    },
    {
     "ename": "ParseSyntaxError",
     "evalue": "Syntax error: no production for mulExpAtom void",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mParseSyntaxError\u001B[0m                          Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_24344\\212411956.py\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfor\u001B[0m \u001B[0ms\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mparse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrammar\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_generator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"testcases/10_编译错误示例/10_lexical.txt\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_24344\\2813444001.py\u001B[0m in \u001B[0;36mparse\u001B[1;34m(grammar, input_generator)\u001B[0m\n\u001B[0;32m     44\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mproduction\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     45\u001B[0m                 \u001B[1;32myield\u001B[0m \u001B[1;34mf\"{top}#{token}\\terror\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 46\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mParseSyntaxError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Syntax error: no production for {top} {token}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     47\u001B[0m             \u001B[1;31m# print(f\"Apply production {production}\")\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m             \u001B[1;32myield\u001B[0m \u001B[1;34mf\"{top}#{token}\\treduction\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mParseSyntaxError\u001B[0m: Syntax error: no production for mulExpAtom void"
     ]
    }
   ],
   "source": [
    "for s in parse(grammar, input_generator(\"testcases/10_编译错误示例/10_lexical.txt\")):\n",
    "    print(s)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Unit test for parsing\n",
    "# \"\"\"\n",
    "# grammar_parse = load_grammar(\"unittest/grammar_parse.txt\")\n",
    "# grammar_parse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for token, lexeme in input_generator(\"unittest/lexical.txt\"):\n",
    "#     print(f\"{token} {lexeme}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# FIRST = get_all_first(grammar_parse)\n",
    "# FIRST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# FOLLOW = get_all_follow(grammar_parse, FIRST)\n",
    "# FOLLOW"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# parsing_table = get_parsing_table(grammar_parse.productions, FIRST, FOLLOW)\n",
    "# with open(\"parsing_table.txt\", \"w\") as f:\n",
    "#     print_parsing_table(grammar_parse, parsing_table, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# parse(grammar_parse, input_generator(\"unittest/lexical.txt\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted\n",
      "Accepted\n",
      "Accepted\n",
      "Accepted\n",
      "Syntax error: no production for varDef INT\n",
      "Syntax error: no production for mulExpAtom void\n"
     ]
    }
   ],
   "source": [
    "for testcase_dir in Path(\"testcases\").glob(\"*\"):\n",
    "    ans_path = next(testcase_dir.glob(\"*_lexical.txt\"))\n",
    "    outfile_name = f\"{ans_path.name[:-4]}_parsing.txt\"  # remove trailing .txt\n",
    "    with open(f\"output/{outfile_name}\", \"w\") as f:\n",
    "        try:\n",
    "            for s in parse(grammar, input_generator(ans_path)):\n",
    "                print(s, file=f)\n",
    "        except ParseSyntaxError as e:\n",
    "            print(e)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 passed!\n",
      "01 passed!\n",
      "02 passed!\n",
      "07 passed!\n",
      "08_编译错误示例 passed!\n",
      "10_编译错误示例 failed!\n",
      "21c21,26\n",
      "< mulExpAtom#void\terror\n",
      "---\n",
      "> mulExpAtom#void\treduction\n",
      "> addExpAtom#void\treduction\n",
      "> relExpAtom#void\treduction\n",
      "> eqExpAtom#void\treduction\n",
      "> assignExpAtom#void\treduction\n",
      "> argVarDecl#void\terror\n"
     ]
    }
   ],
   "source": [
    "for testcase_dir in Path(\"testcases\").glob(\"*\"):\n",
    "    ans_path = next(testcase_dir.glob(\"*_grammar.txt\"))\n",
    "    res_path = f'output/{ans_path.name.replace(\"grammar.txt\",\"lexical_parsing.txt\")}'\n",
    "\n",
    "    # compare\n",
    "    with open(res_path) as f:\n",
    "        # strip trailing cr\n",
    "        output = [line.rstrip() for line in f.readlines()]\n",
    "    with open(ans_path) as f:\n",
    "        # expected = f.readlines()\n",
    "        expected = [line.rstrip() for line in f.readlines()]\n",
    "    if output == expected:\n",
    "        print(f\"{testcase_dir.name} passed!\")\n",
    "    else:\n",
    "        print(f\"{testcase_dir.name} failed!\")\n",
    "        !diff {res_path} {ans_path}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
